dataset:
  name: "BDD100K"
  root_dir: "/mnt/team-share/Datasets/bdd-datasets/BDD/bdd_100k/"
  train_split: "images/100k/train/"
  val_split: "images/100k/val/"
  test_split: "images/100k/test/"
  train_ann_file: "labels/bdd100k_labels_images_train.json"
  val_ann_file: "labels/bdd100k_labels_images_val.json"
  test_ann_file: "labels/bdd100k_labels_images_test.json"
  convert_to_coco: true
  export_coco_path: "/mnt/team-share/Datasets/bdd-datasets/BDD/bdd_100k/coco_format/"

model:
  name: "gai-yolo12-large-003"
  pretrained: true
  num_classes: 10
  input_size: [640, 640]
  backbone: "tiny_csp"          # Backbone architecture: "tiny_csp"
  backbone_params:               # Backbone-specific parameters
    base_channels: 128            # Base channel width (32, 64, 128, 256)
    depth: 4                     # Number of stages (2, 3, 4)
  head:                          # Detection head configuration
    hidden_channels: 256         # Hidden layer channels in detection head (128, 256, 512)
  
  # Fine-tuning configuration
  freeze_backbone: true          # Freeze backbone initially for faster convergence
  freeze_neck: false             # Train neck/FPN layers
  freeze_until_epoch: 10         # Unfreeze backbone after epoch 10
  backbone_lr_multiplier: 0.1    # Backbone learns 10x slower (when unfrozen)
  neck_lr_multiplier: 0.5        # Neck learns 2x slower

dataloader:
  batch_size: 128
  num_workers: 20
  prefetch_factor: 4
  persistent_workers: false
  pin_memory: true
  augmentation: true             # Enable light augmentation for fine-tuning
  augmentation_strategies:       # Light augmentation only
    - "RandomHorizontalFlip"
    - "ColorJitter"
  augmentation_probabilities_per_batch: 0.05  # Very light: 5% of batches
  augmentations:
    - type: "RandomHorizontalFlip"
      probability: 0.5
    - type: "ColorJitter"        # BDD100K has varying lighting conditions
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1

experiment:
  name: "gai-yolov12-large-experiment-003"
  description: "Fine-tuning GAI-YOLOv12 on BDD100K with warmup scheduler and layer freezing"
  num_epochs: 100
  learning_rate: 0.0001          # Increased from 1e-5 for better convergence
  weight_decay: 0.001
  
  # Learning rate scheduler with warmup (recommended for fine-tuning)
  scheduler: "cosine_warmup"
  scheduler_params:
    warmup_epochs: 5             # Gentle warmup for 5 epochs
    warmup_bias_lr: 0.01         # Start at 1% of target LR
    eta_min: 0.000001            # End at 1e-6 (10% of initial)
  
  max_batches_per_epoch: null
  checkpoint_interval: 2
  gradient_clip_norm: 10.0
  log_interval: 10
  use_amp: true
  tensorboard:
    enabled: true
    log_interval: 10
    log_images: true
    images_per_batch: 2
    image_log_interval: 100  # Log images every 100 batches
    show_ground_truth: false  # Show ground truth boxes in visualizations
    show_predictions: true   # Show prediction boxes in visualizations
  evaluation:
    enabled: true
    eval_interval: 5  # Evaluate COCO metrics every 5 epochs
    eval_on_validation: true
    visualization_conf_threshold: 0.5  # Confidence threshold for showing predictions (0.01-1.0)
